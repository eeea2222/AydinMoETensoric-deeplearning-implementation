import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
import time

# --- 1. AYDIN CONFIG (Sistemin Kalbi) ---
@dataclass
class AydinConfig:
    hidden_dim: int = 512
    intermediate_dim: int = 1024
    num_experts: int = 8      # Tek tensÃ¶rde 8 uzman
    top_k: int = 2            # Her token iÃ§in 2 aktif uzman
    dropout: float = 0.1

# --- 2. TENSORIC ENGINE: The Vectorized Expert Layer ---
class TensoricSwiGLU(nn.Module):
    """
    Pythonic DEÄÄ°L, Tensoric!
    UzmanlarÄ± dÃ¶ngÃ¼yle gezmek yerine, aÄŸÄ±rlÄ±klarÄ± (Experts, In, Out) ÅŸeklinde
    3 boyutlu tensÃ¶rlerde tutar ve 'gather' ile iÅŸlem yapar.
    """
    def __init__(self, config: AydinConfig):
        super().__init__()
        self.hidden_dim = config.hidden_dim
        self.inter_dim = config.intermediate_dim
        self.num_experts = config.num_experts
        
        # TÃ¼m uzmanlarÄ±n aÄŸÄ±rlÄ±klarÄ± tek bir devasa tensÃ¶rde!
        # Shape: (Num_Experts, Hidden_Dim, Intermediate_Dim * 2) -> Gate + Up projeksiyonlarÄ± birleÅŸik
        self.w13 = nn.Parameter(torch.empty(config.num_experts, config.hidden_dim, config.intermediate_dim * 2))
        
        # Shape: (Num_Experts, Intermediate_Dim, Hidden_Dim) -> Down projeksiyonu
        self.w2 = nn.Parameter(torch.empty(config.num_experts, config.intermediate_dim, config.hidden_dim))
        
        self.reset_parameters()

    def reset_parameters(self):
        # Xavier/Kaiming yerine basit normal init (HÄ±z iÃ§in)
        nn.init.normal_(self.w13, std=0.02)
        nn.init.normal_(self.w2, std=0.02)

    def forward(self, x: torch.Tensor, expert_indices: torch.Tensor) -> torch.Tensor:
        """
        x: (Batch, Seq, Hidden)
        expert_indices: (Batch, Seq, TopK) -> Router'dan gelen indeksler
        """
        # 1. AÄŸÄ±rlÄ±k SeÃ§imi (Weight Gathering)
        # Bu adÄ±m sihirli: Her token iÃ§in gerekli olan uzman matrislerini "Ã§ekiyoruz".
        # w13 shape: (E, H, 2*I) -> SeÃ§ilen: (B, S, TopK, H, 2*I)
        # Bu iÅŸlem GPU belleÄŸinde bant geniÅŸliÄŸi ister ama hesaplamayÄ± %100 vektÃ¶rize eder.
        selected_w13 = self.w13[expert_indices] 
        
        # 2. VektÃ¶rize Matris Ã‡arpÄ±mÄ± (Input Projection)
        # x'i (B, S, 1, 1, H) yaparak boyutlarÄ± hizalÄ±yoruz.
        # SonuÃ§: (B, S, TopK, 1, 2*I)
        x_expanded = x.unsqueeze(2).unsqueeze(2)
        h13 = torch.matmul(x_expanded, selected_w13) 
        
        # 3. SwiGLU Aktivasyonu (Fused)
        # Split gate & up
        gate, up = h13.split(self.inter_dim, dim=-1)
        h_inter = F.silu(gate) * up # (B, S, TopK, 1, I)
        
        # 4. Down Projection (Ã‡Ä±kÄ±ÅŸ)
        selected_w2 = self.w2[expert_indices] # (B, S, TopK, I, H)
        out = torch.matmul(h_inter, selected_w2) # (B, S, TopK, 1, H)
        
        # Gereksiz boyutlarÄ± at: (B, S, TopK, H)
        return out.squeeze(-2)

# --- 3. ARCHITECT: AydinMoE "Tensoric" Edition ---
class AydinMoETensoric(nn.Module):
    def __init__(self, config: AydinConfig):
        super().__init__()
        self.config = config
        
        # Router (Hala basit bir Linear katman, ama hÄ±zlÄ±)
        self.router = nn.Linear(config.hidden_dim, config.num_experts, bias=False)
        
        # Tek bir devasa modÃ¼l (Liste yok!)
        self.experts = TensoricSwiGLU(config)

    def forward(self, x: torch.Tensor):
        # x: (Batch, Seq, Hidden)
        
        # --- 1. ROUTING ---
        router_logits = self.router(x)
        routing_probs = F.softmax(router_logits, dim=-1)
        
        # Top-K seÃ§imi (Indices ve Weights)
        # indices: (B, S, K), weights: (B, S, K)
        weights, indices = torch.topk(routing_probs, self.config.top_k, dim=-1)
        
        # Normalize weights
        weights = weights / weights.sum(dim=-1, keepdim=True)
        
        # --- 2. TENSORIC COMPUTATION ---
        # DÃ¶ngÃ¼ yok, stream yok. Sadece saf matematik.
        # expert_output: (B, S, TopK, H)
        expert_output = self.experts(x, indices)
        
        # --- 3. WEIGHTED COMBINATION ---
        # Her uzmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ± router aÄŸÄ±rlÄ±ÄŸÄ± ile Ã§arp:
        # weights.unsqueeze(-1) -> (B, S, K, 1)
        weighted_output = expert_output * weights.unsqueeze(-1)
        
        # Topla (Reduce over TopK dimension) -> (B, S, H)
        final_output = weighted_output.sum(dim=2)
        
        return final_output

# --- 4. THE COMPILER SETUP (Ålak Diye YapÄ±ÅŸtÄ±rma BÃ¶lÃ¼mÃ¼) ---
def get_compiled_model():
    conf = AydinConfig()
    model = AydinMoETensoric(conf).cuda().to(dtype=torch.bfloat16) # bfloat16 candÄ±r
    
    # NVIDIA GPU'larda Tensor Core'larÄ± zorla
    torch.set_float32_matmul_precision('high')
    
    print("ğŸš€ Model derleniyor (torch.compile)... Ä°lk Ã§alÄ±ÅŸtÄ±rma biraz sÃ¼rebilir.")
    # mode="max-autotune": Triton kernel'larÄ±nÄ± en dibine kadar optimize eder.
    # fullgraph=True: Graph break (grafik kopmasÄ±) istemiyoruz!
    compiled_model = torch.compile(model, mode="max-autotune")
    
    return compiled_model

# --- 5. BENCHMARK ---
if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("Bu kod sadece NVIDIA GPU ile gerÃ§ek gÃ¼cÃ¼nÃ¼ gÃ¶sterir!")
        exit()

    model = get_compiled_model()
    
    # Dummy Data (BÃ¼yÃ¼k Batch)
    # (32 Batch, 128 Seq, 512 Hidden)
    x = torch.randn(32, 128, 512, device="cuda", dtype=torch.bfloat16)
    
    # 1. Warmup (Derleme burada gerÃ§ekleÅŸir)
    print("ğŸ”¥ IsÄ±nÄ±yor (Compiling kernels)...")
    start = time.time()
    for _ in range(3):
        _ = model(x)
    print(f"IsÄ±nma Bitti: {time.time() - start:.2f} sn (Derleme dahil)")

    # 2. HÄ±z Testi
    print("\nâš¡ BENCHMARK BAÅLIYOR (1000 iterasyon) âš¡")
    
    torch.cuda.synchronize()
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    
    start_event.record()
    
    # HiÃ§bir Python dÃ¶ngÃ¼sÃ¼ GPU'yu bekletmesin diye graph capture kullanabiliriz 
    # ama ÅŸimdilik saf compile performansÄ±na bakalÄ±m.
    for _ in range(1000):
        _ = model(x)
        
    end_event.record()
    torch.cuda.synchronize()
    
    elapsed_ms = start_event.elapsed_time(end_event)
    print(f"Toplam SÃ¼re: {elapsed_ms:.2f} ms")
    print(f"Ä°terasyon BaÅŸÄ±na: {elapsed_ms/1000:.3f} ms")
    print("\nSonuÃ§: Bu mimari statiktir. torch.compile tÃ¼m uzman hesaplamalarÄ±nÄ±")
    print("tek bir 'fused kernel' haline getirir. Google mÃ¼hendisleri bunu beÄŸendi. ğŸ˜")